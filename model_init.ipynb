{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Model Initialization\n",
    "\n",
    "In this notebook, we study the initialization of very large models. There are many types of optimization and different ways of implementation for large model initialization. Here we want to study their impact on\n",
    "\n",
    "1. peak memory\n",
    "2. running time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Footprint\n",
    "\n",
    "Suppose we have a pretrained checkpoint of a model. Then the peak memory of model initialization can be reduced by\n",
    "\n",
    "1. checkpoint sharding\n",
    "2. deferred model materialzation\n",
    "3. data type (model quantization)\n",
    "\n",
    "A typical model initialization process is 1) materialize a model with random weights; 2) load in the checkpointed weights; 3) assign the weights to the model. Unfortunately, the peak memory of such process is `twice the model size` (random weights + checkpointed weights).\n",
    "\n",
    "One way to remedy this is to use sharded checkpoints. We load in one shard of the checkpoint at a time and assign the weights accordingly. This process has the peak memory of `model size + shard size`.\n",
    "\n",
    "To achieve a peak memory of `model size`, we can first initialize the model with a empty shell and materialize it directly with the loaded checkpoints. This requires initialization with `meta device` in PyTorch.\n",
    "\n",
    "Here we compare these ideas using two implementations: HuggingFace and native PyTorch.\n",
    "\n",
    "> _NOTE:_ We will not use `torchdistx` and its `deferred_init`, as it does not work with PyTorch 2.0+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tracemalloc\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original memory usage is 0.000704MB; Current memory usage is 0.259698MB; Peak was 0.591343MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tracemalloc.start()\n",
    "\n",
    "# your code here\n",
    "\n",
    "original, _ = tracemalloc.get_traced_memory()\n",
    "\n",
    "llama_weight_path = Path('/project/llama/7B')\n",
    "# Create a sharded version of the original parameter\n",
    "weights = torch.load(\n",
    "    llama_weight_path / 'consolidated.00.pth', map_location='cpu')\n",
    "\n",
    "current, peak = tracemalloc.get_traced_memory()\n",
    "\n",
    "print(f\"Original memory usage is {original / 10**6}MB; Current memory usage is {current / 10**6}MB; Peak was {peak / 10**6}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0199,  0.0213,  0.0250,  ..., -0.0610,  0.0007, -0.0143],\n",
       "        [ 0.0409,  0.0204,  0.0125,  ...,  0.0070, -0.0222,  0.0151],\n",
       "        [ 0.0217,  0.0078,  0.0133,  ...,  0.0112,  0.0403,  0.0081],\n",
       "        ...,\n",
       "        [-0.0366,  0.0066,  0.0679,  ..., -0.0173, -0.0131,  0.0312],\n",
       "        [ 0.0116, -0.0162,  0.0045,  ...,  0.0458,  0.0015, -0.0046],\n",
       "        [ 0.0330,  0.0108, -0.0049,  ..., -0.0088,  0.0036, -0.0050]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['layers.30.feed_forward.w2.weight']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
